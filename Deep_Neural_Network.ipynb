{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Deep_Neural_Network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4qC1ImhE9xT",
        "colab_type": "text"
      },
      "source": [
        "# Deep Neural Network for Credit Card Fraud-Detection - Application\n",
        "\n",
        "The dataset used was downloaded from https://www.kaggle.com/mlg-ulb/creditcardfraud.\n",
        "It contained transactions done in September 2013. It contains only numeric input variables as a \n",
        "result of PCA transformation due to confidentiality issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuMJTXYCE9xV",
        "colab_type": "text"
      },
      "source": [
        "1. Importing all the packages required for the project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mCql13JE9xe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtBWjFWZE9xj",
        "colab_type": "text"
      },
      "source": [
        "2. Reading in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OywWm52FE9xl",
        "colab_type": "code",
        "outputId": "1a427850-587d-40cd-c96b-7c4bd326c63f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "credit_data = pd.read_csv(r'/content/creditcard.csv')\n",
        "print(credit_data.shape)\n",
        "credit_data.head()"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(284807, 31)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
              "0   0.0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62      0\n",
              "1   0.0  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69      0\n",
              "2   1.0 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66      0\n",
              "3   1.0 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  123.50      0\n",
              "4   2.0 -1.158233  0.877737  1.548718  ...  0.219422  0.215153   69.99      0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQLfdLqwE9xp",
        "colab_type": "text"
      },
      "source": [
        "3. Creating train-test split of 75-25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkGQqViCE9xr",
        "colab_type": "code",
        "outputId": "70b6d5da-0698-4450-a8d5-1dea3e5cdcec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = credit_data.iloc[:, 1:30]\n",
        "Y = credit_data['Class']\n",
        "\n",
        "print('X : ', X.head())\n",
        "print('Y : ', Y.head())"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X :           V1        V2        V3        V4  ...       V26       V27       V28  Amount\n",
            "0 -1.359807 -0.072781  2.536347  1.378155  ... -0.189115  0.133558 -0.021053  149.62\n",
            "1  1.191857  0.266151  0.166480  0.448154  ...  0.125895 -0.008983  0.014724    2.69\n",
            "2 -1.358354 -1.340163  1.773209  0.379780  ... -0.139097 -0.055353 -0.059752  378.66\n",
            "3 -0.966272 -0.185226  1.792993 -0.863291  ... -0.221929  0.062723  0.061458  123.50\n",
            "4 -1.158233  0.877737  1.548718  0.403034  ...  0.502292  0.219422  0.215153   69.99\n",
            "\n",
            "[5 rows x 29 columns]\n",
            "Y :  0    0\n",
            "1    0\n",
            "2    0\n",
            "3    0\n",
            "4    0\n",
            "Name: Class, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFAeePpSE9xx",
        "colab_type": "code",
        "outputId": "d8fad7e1-aa9a-4113-c944-513bf194a7c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 1234)\n",
        "print('Shape of X_train : ', X_train.shape)\n",
        "print('Shape of X_test : ', X_test.shape)\n",
        "print('Shape of Y_train : ', Y_train.shape)\n",
        "print('Shape of Y_test : ', Y_test.shape)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_train :  (213605, 29)\n",
            "Shape of X_test :  (71202, 29)\n",
            "Shape of Y_train :  (213605,)\n",
            "Shape of Y_test :  (71202,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va0GYTSdE9x2",
        "colab_type": "text"
      },
      "source": [
        "4. Sampling to handle imbalanced target label - under, over and SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4IWat3KBFja",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "e677396a-ea74-42dd-b7ba-1ba99df8fc8b"
      },
      "source": [
        "#Under-sampling\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "rus = RandomUnderSampler(random_state=0)\n",
        "X_under_sampled, y_under_sampled = rus.fit_resample(X_train, Y_train)\n",
        "\n",
        "#Over-sampling\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "ros = RandomOverSampler(random_state=0)\n",
        "X_over_sampled, y_over_sampled = ros.fit_resample(X_train, Y_train)\n",
        "\n",
        "#SMOTE\n",
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE(random_state=2)\n",
        "X_smote, y_smote = sm.fit_sample(X_train, Y_train)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR04agUYBV8A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2072291a-29ae-4237-cc51-50fa0aa5e1ec"
      },
      "source": [
        "#Check the new shape of each training sample\n",
        "\n",
        "print('Shape of under-sampled train dataset : ', X_under_sampled.shape)\n",
        "print('Shape of over-sampled train dataset : ', X_over_sampled.shape)\n",
        "print('Shape of SMOTE train dataset : ', X_smote.shape)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of under-sampled train dataset :  (748, 29)\n",
            "Shape of over-sampled train dataset :  (426462, 29)\n",
            "Shape of SMOTE train dataset :  (426462, 29)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2aVgBm2BXBq",
        "colab_type": "text"
      },
      "source": [
        "The under-sampled dataset contains very few data points, adn hence will nt be used for modeling. The next step is to chosse between over-sampled and SMOTE dataset. We will run a DNN model on both and see which one performs better\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vcl1PqtVIaFX",
        "colab_type": "text"
      },
      "source": [
        "5. Convert the training dataset (both predictors as well as target variable) to an N-dimensional array of size (29, 142144) for vectorized implementation of neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_7vpg55Hds9",
        "colab_type": "code",
        "outputId": "1973c558-067f-4daf-b643-88e8f2a1db0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Convert the training samples to ND-arrays for creating a DNN from scratch.\n",
        "print('SMOTE dataset :')\n",
        "features_smote = X_smote.T\n",
        "print(features_smote.shape)\n",
        "\n",
        "labels_smote = y_smote.reshape(-1,1).T\n",
        "print(labels_smote.shape)\n"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SMOTE dataset :\n",
            "(29, 426462)\n",
            "(1, 426462)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeT5bc2kNd-t",
        "colab_type": "text"
      },
      "source": [
        "6. Defining sigmoid and RELU function for implementation in the later part\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5goqqWSNNi2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(Z):\n",
        "\n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "    return A, cache\n",
        "\n",
        "def relu(Z):\n",
        "   \n",
        "    A = np.maximum(0,Z)\n",
        "    assert(A.shape == Z.shape)\n",
        "    cache = Z \n",
        "    return A, cache\n",
        "\n",
        "\n",
        "def relu_backward(dA, cache):\n",
        "    \n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) \n",
        "  \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "def sigmoid_backward(dA, cache):\n",
        "   \n",
        "    Z = cache\n",
        "    \n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2x9EIJcLJk0h",
        "colab_type": "text"
      },
      "source": [
        "7. Initializing parameters - the linear forward part of the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbBaProyJnYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters_deep(layer_dims):\n",
        "    \n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)           \n",
        "\n",
        "    for l in range(1, L):\n",
        "       \n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
        "        parameters['b' + str(l)] =  np.zeros((layer_dims[l], 1))\n",
        "       \n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUVhQOzWMHtP",
        "colab_type": "text"
      },
      "source": [
        "8. Doing the forward propagation \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg3YFXWMMKw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This is the linear forward model\n",
        "def linear_forward(A, W, b):\n",
        "\n",
        "    Z = np.dot(W, A) + b\n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    return Z, cache\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XNyE_Q_OLtn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This is the linear activation forward model\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "   \n",
        "    if activation == \"sigmoid\":\n",
        "       \n",
        "        Z, linear_cache  = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "        \n",
        "    elif activation == \"relu\":\n",
        "       \n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)\n",
        "       \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhQpr4R1MqCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Defining a final forward model to implement the linear as well as activation functions\n",
        "def L_model_forward(X, parameters):\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  \n",
        "\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "      \n",
        "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
        "        caches.append(cache)\n",
        "\n",
        "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
        "    caches.append(cache)\n",
        "   \n",
        "    assert(AL.shape == (1,X.shape[1]))\n",
        "            \n",
        "    return AL, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcxrPr5b2DaV",
        "colab_type": "text"
      },
      "source": [
        "9. Computing the cost function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhanaXo32JbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_cost(AL, Y):\n",
        "   \n",
        "    m = Y.shape[1]\n",
        "    logprobs = np.multiply(np.log(AL),Y) +  np.multiply(np.log(1-AL), (1-Y))\n",
        "    cost = -1/m*np.sum(logprobs)\n",
        "    cost = np.squeeze(cost)      \n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBM4bF9E39MX",
        "colab_type": "text"
      },
      "source": [
        "10. The next step is to do backward propagation based on the output of cost function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceFt1f0d4C2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Linear backward model\n",
        "def linear_backward(dZ, cache):\n",
        "   \n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "    dW = 1./m*np.dot(dZ, A_prev.T)\n",
        "    db = 1./m*np.sum(dZ, axis = 1, keepdims=True)\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8wtlCe34ywm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Linear backward activation model\n",
        "def linear_activation_backward(dA, cache, activation):\n",
        "   \n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    elif activation == \"sigmoid\":\n",
        "       \n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "      \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwiMuuBR7Q6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "    \n",
        "    grads = {}\n",
        "    L = len(caches) \n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) \n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
        "    for l in reversed(range(L-1)):\n",
        "       \n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)],  current_cache, activation = \"relu\")\n",
        "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k9DiCCq7dDg",
        "colab_type": "text"
      },
      "source": [
        "11. Updating parameters - weight and bias for each layer, each unit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obsmvuZK7jgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \n",
        "    L = len(parameters) // 2 \n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]  \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chZuPUcjNcUr",
        "colab_type": "text"
      },
      "source": [
        "12. Implement the full L-layer model\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lJ3ONQY8DZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
        "\n",
        "    np.random.seed(1)\n",
        "    costs = []                        \n",
        "\n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "  \n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "       \n",
        "        AL, caches = L_model_forward(X, parameters)\n",
        "        cost = compute_cost(AL, Y)\n",
        "        grads = L_model_backward(AL, Y, caches)\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "            \n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per hundreds)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXVFWHYG8RA5",
        "colab_type": "code",
        "outputId": "9ab811ee-19fc-420b-ca73-eb8f9a56ba63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 805
        }
      },
      "source": [
        "dimensions = [features_smote.shape[0], 24, 20, 5, labels_smote.shape[0]]\n",
        "parameters = L_layer_model(features_smote, labels_smote, dimensions, num_iterations = 3000, print_cost = True)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.693152\n",
            "Cost after iteration 100: 0.693152\n",
            "Cost after iteration 200: 0.693152\n",
            "Cost after iteration 300: 0.693151\n",
            "Cost after iteration 400: 0.693151\n",
            "Cost after iteration 500: 0.693151\n",
            "Cost after iteration 600: 0.693150\n",
            "Cost after iteration 700: 0.693150\n",
            "Cost after iteration 800: 0.693150\n",
            "Cost after iteration 900: 0.693149\n",
            "Cost after iteration 1000: 0.693149\n",
            "Cost after iteration 1100: 0.693149\n",
            "Cost after iteration 1200: 0.693149\n",
            "Cost after iteration 1300: 0.693149\n",
            "Cost after iteration 1400: 0.693148\n",
            "Cost after iteration 1500: 0.693148\n",
            "Cost after iteration 1600: 0.693148\n",
            "Cost after iteration 1700: 0.693148\n",
            "Cost after iteration 1800: 0.693148\n",
            "Cost after iteration 1900: 0.693148\n",
            "Cost after iteration 2000: 0.693148\n",
            "Cost after iteration 2100: 0.693148\n",
            "Cost after iteration 2200: 0.693147\n",
            "Cost after iteration 2300: 0.693147\n",
            "Cost after iteration 2400: 0.693147\n",
            "Cost after iteration 2500: 0.693147\n",
            "Cost after iteration 2600: 0.693147\n",
            "Cost after iteration 2700: 0.693147\n",
            "Cost after iteration 2800: 0.693147\n",
            "Cost after iteration 2900: 0.693147\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaIAAAEWCAYAAAAkUJMMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQjnDRomvbpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(parameters, X):\n",
        "   \n",
        "    A2, cache = L_model_forward(X, parameters)\n",
        "    predictions = np.round(A2)\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aD2L5g5uWH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_train = predict(parameters, features)\n",
        "pred_test = predict(parameters, np.asarray(X_test).T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8uhAF67qIzx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3bd88056-7fcf-4f1a-b564-89866d397339"
      },
      "source": [
        "#Computing accuracy and sensitivity for the predicted validation set\n",
        "actual_labels = np.asarray(Y_test).reshape(1,-1)\n",
        "predicted_labels = pred_test\n",
        "\n",
        "TP = np.squeeze(np.dot(actual_labels,predicted_labels.T))\n",
        "TN = np.squeeze(np.dot(1-actual_labels,1-predicted_labels.T))\n",
        "FN = np.squeeze(actual_labels[np.where(actual_labels == 0)].shape[0]  -   np.dot(1-actual_labels,1-predicted_labels.T))\n",
        "\n",
        "accuracy  = (TP + TN)/(actual_labels.size)\n",
        "sensitivity = np.round((TP*100/(TP+FN)),2)\n",
        "print ('Accuracy: %d' % float((np.dot(actual_labels,predicted_labels.T) + np.dot(1-actual_labels,1-predicted_labels.T))/float(actual_labels.size)*100) + '%')\n",
        "print(sensitivity)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 17%\n",
            "0.17\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}